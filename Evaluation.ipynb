{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsme\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get image information\n",
    "import google.generativeai as genai\n",
    "import config\n",
    "def input_image_setup(file_loc):\n",
    "    from pathlib import Path\n",
    "\n",
    "    if not (img := Path(file_loc)).exists():\n",
    "        raise FileNotFoundError(f\"Could not find image: {img}\")\n",
    "\n",
    "    image_parts = [\n",
    "        {\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "            \"data\": Path(file_loc).read_bytes()\n",
    "            }\n",
    "        ]\n",
    "    return image_parts\n",
    "def get_image_info(image_loc, prompt):\n",
    "    genai.configure(api_key=config.gemini_key)\n",
    "    # Set up the model\n",
    "    generation_config = {\n",
    "        \"temperature\":0,\n",
    "        \"top_p\":1,\n",
    "        \"top_k\":32,\n",
    "        \"max_output_tokens\":4096,\n",
    "    }\n",
    "    \n",
    "    model = genai.GenerativeModel(model_name=\"gemini-pro-vision\", generation_config=generation_config)\n",
    "\n",
    "    input_prompt = \"\"\" You are expert in Evaulation Graphs \"\"\"\n",
    "\n",
    "    question_prompt = prompt\n",
    "\n",
    "    image_prompt = input_image_setup(image_loc)\n",
    "    prompt_parts = [input_prompt, image_prompt[0], question_prompt]\n",
    "    response = model.generate_content(prompt_parts)\n",
    "    return str(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert tasked with evaluating the Python code generated by an LLM. The LLM could generate Python code \n",
      "for chart figure based on input raw csv data and instructions. I will provide you with the input raw data and the \n",
      "instruction. Also, I will give you a reference code, and the predicted python code by the LLM. You need to evaluate the \n",
      "predicted Python code and score it from 0 points to 5 points. Here is the criteria:\n",
      "1. **Correctness**: This metric evaluates whether the generated code accurately fulfills the given instructions. The \n",
      "score could be binary (1 for correct, 0 for incorrect) or based on a proportion of test cases passed.\n",
      "Scoring Standard: \n",
      "- Score 5: The code fulfills all tasks perfectly.\n",
      "- Score 3: The code fulfills some tasks but has minor errors.\n",
      "- Score 0: The code does not fulfill the tasks or is entirely incorrect.\n",
      "2. **Readability**: This metric assesses whether the code is easy to read and understand, which includes appropriate \n",
      "use of comments, variable names, and code structure.\n",
      "Scoring Standard: \n",
      "- Score 5: The code is very readable with good structure, comments, and variable names.\n",
      "- Score 3: The code is somewhat readable but could be improved.\n",
      "- Score 0: The code is not readable or poorly structured.\n",
      "3. **Visualization Aesthetics and Detailing**: This metric evaluates the level of detailing in the generated figures and \n",
      "the aesthetics of the visualization. It assesses how well the code incorporates elements like color, labels, annotations, \n",
      "and other features to improve the look and interpretability of the graphs.\n",
      "Scoring Standard:\n",
      "- Score 5: The code consistently generates figures with excellent detailing and aesthetics. Graphs have appropriate \n",
      "and diverse color schemes, clear labels, and annotations, making them easy to interpret and visually appealing.\n",
      "- Score 3: The code generates figures with adequate detailing and aesthetics. Some elements like color, labels, or \n",
      "annotations could be improved for better interpretability and visual appeal.\n",
      "- Score 0: The code does not generate figures, or the figures generated lack any form of detailing or aesthetics, \n",
      "making them uninterpretable and visually unappealing.\n",
      "The raw data and instruction:\n",
      "[Raw tabular data and Instructions]\n",
      "The reference Python code:\n",
      "[Reference code]\n",
      "The predicted Python code:\n",
      "[Predicted code]\n",
      "The output should first give the average score based on three criteria, then output scores for each criteria. The output \n",
      "should follow this format:\n",
      "Average: number \n",
      "Correctness: number\n",
      "Readability: number\n",
      "Visualization Aesthetics and Detailing: number\n"
     ]
    }
   ],
   "source": [
    "with open('prompts\\Evaluate_code_Generated_to_plot_graph_Avinash.txt') as prompt:\n",
    "    prompt = prompt.read()\n",
    "    print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Average: 3.0\\nCorrectness: 5\\nReadability: 3\\nVisualization Aesthetics and Detailing: 3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_loc = 'replot_chart.png'\n",
    "get_image_info(image_loc,prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
